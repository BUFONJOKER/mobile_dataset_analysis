
---

# üì± **Mobile Dataset Analysis and Visualization**

## üß≠ **Project Overview**

The **Mobile Dataset Analysis and Visualization** project focuses on understanding, cleaning, and modeling data related to smartphones. The goal is to extract **valuable insights** about mobile features, performance, and pricing, and to build predictive models that can help in decision-making ‚Äî for instance, predicting mobile prices or identifying factors influencing high-end phones.

This project follows a complete **Data Science Lifecycle**, including:

* **Data Preprocessing** ‚Äì preparing the raw dataset for analysis
* **Exploratory Data Analysis (EDA)** ‚Äì discovering hidden trends and relationships
* **Model Building and Evaluation** ‚Äì creating and assessing machine learning models
* **Deployment** ‚Äì making the final model accessible through an interactive web interface

---

## ‚öôÔ∏è **Workflow**

### 1. üßπ **Data Preprocessing**

Data preprocessing ensures that the dataset is accurate, consistent, and ready for analysis.
Key steps include:

* **Handling Missing Values:**
  Missing data is identified using functions like `df.isnull().sum()` and handled through imputation (mean/median/mode) or row/column removal, depending on data importance.

* **Removing Duplicates:**
  Duplicated entries (such as repeated models or same-price records) are detected using `df.duplicated()` and dropped to avoid bias.

* **Data Type Conversion:**
  Columns like ‚ÄúLaunched Price‚Äù may contain symbols (e.g., `$`, `‚Çπ`), which are cleaned using regular expressions before converting them to numerical data types.

* **Feature Engineering:**

  * Creating new features (e.g., *Pixel Density*, *Price-to-Performance Ratio*).
  * Encoding categorical variables (e.g., *Company Name*, *Processor*) using one-hot encoding or label encoding.

* **Normalization and Scaling:**
  Scaling numeric columns (like *RAM*, *Battery Capacity*, *Screen Size*) ensures that large-scale features do not dominate small-scale ones in model training.

---

### 2. üîç **Exploratory Data Analysis (EDA)**

EDA helps uncover patterns and relationships within the dataset. Visualizations are created using **Matplotlib** and **Seaborn**.

* **Univariate Analysis (Single Feature):**

  * Histograms and KDE plots show feature distributions (e.g., *Price*, *Battery Capacity*).
  * Boxplots detect outliers.
    Example: `sns.boxplot(x='Battery Capacity', data=df)`

* **Bivariate Analysis (Two Features):**

  * Scatter plots reveal relationships between *Screen Size* and *Launched Price*.
  * Bar charts compare *Average Price* by *Company Name*.
    Example: `sns.barplot(x='Company Name', y='Launched Price', data=df)`

* **Multivariate Analysis:**

  * Heatmaps display correlations between numerical features.
  * Pairplots visualize feature interactions.
    Example: `sns.heatmap(df.corr(), annot=True, cmap='coolwarm')`

* **Trend Analysis:**
  Detects which features most influence pricing ‚Äî e.g., *Processor Type* and *RAM Size* often have strong positive correlations with *Price*.

---

### 3. ü§ñ **Model Building**

This stage involves building predictive models using **Scikit-learn**. The process includes:

* **Feature Selection:**
  Identifying the most relevant features through correlation analysis, recursive feature elimination (RFE), or tree-based feature importance.

* **Splitting Dataset:**
  Using `train_test_split()` to divide data into training and testing sets for unbiased model evaluation.

* **Algorithm Selection:**
  Different models are trained based on the problem:

  * **Regression:** Linear Regression, Decision Tree Regressor, Random Forest Regressor
  * **Classification (if applicable):** Logistic Regression, SVM, KNN, etc.

* **Hyperparameter Tuning:**
  Applying GridSearchCV or RandomizedSearchCV to optimize parameters for better accuracy and reduced overfitting.

Example:

```python
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
grid.fit(X_train, y_train)
```

---

### 4. üìä **Model Evaluation**

After training, models are tested using various evaluation metrics to determine their effectiveness.

* **Regression Metrics:**

  * Mean Absolute Error (MAE)
  * Mean Squared Error (MSE)
  * R¬≤ Score

* **Classification Metrics (if classification task):**

  * Accuracy
  * Precision, Recall, F1-Score
  * Confusion Matrix and ROC-AUC Curve

* **Model Comparison:**
  All trained models are compared side-by-side to identify the most efficient and accurate one.

Example:

```python
from sklearn.metrics import r2_score, mean_absolute_error
print("R¬≤ Score:", r2_score(y_test, y_pred))
```

---

### 5. üöÄ **Model Deployment**

Once the best model is selected, it is deployed using **Streamlit**, allowing users to interact with it via a web interface.

* **Model Serialization:**
  The model is saved using `pickle` or `joblib` for later use:

  ```python
  import joblib
  joblib.dump(model, 'final_model.pkl')
  ```

* **Building the Streamlit App:**
  Users can input features (e.g., RAM, Camera, Battery) and get a predicted price or classification instantly.

* **Deployment and Maintenance:**
  The app can be hosted on **Streamlit Cloud**, **Heroku**, or **Render**, with monitoring to track performance over time.

---

## üõ†Ô∏è **Technologies Used**

| Category                 | Tools / Libraries               |
| ------------------------ | ------------------------------- |
| **Programming Language** | Python                          |
| **Data Handling**        | Pandas, NumPy                   |
| **Visualization**        | Matplotlib, Seaborn             |
| **Machine Learning**     | Scikit-learn                    |
| **Model Deployment**     | Streamlit                       |
| **Environment**          | Jupyter Notebook / Google Colab |

---

## ‚úÖ **Conclusion**

This project demonstrates a **comprehensive end-to-end data science workflow**, from raw data to a deployable machine learning model.

Key takeaways include:

* Data cleaning and preprocessing are crucial for reliable results.
* EDA provides a clear understanding of feature relationships and their impact on price.
* Machine learning models can effectively predict smartphone prices or classify devices into segments.
* Streamlit offers a user-friendly interface for model deployment, making insights accessible to non-technical users.

Overall, the project combines **data engineering, statistical analysis, visualization, and predictive modeling** to provide actionable insights into the mobile market.

---

